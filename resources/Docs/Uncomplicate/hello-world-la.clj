[{:hello {:label "Hello World LA", :opts {:order :row, :eltsper 1, :size "auto", :wrapfn {:tid :hello, :$split nil, :out-width "1300px", :fn [quote editor-repl-tab], :layout :left-right, :ns hwla.code, :ed-out-order :first-last, :width "730px", :md-defaults nil, :src "\n;;; All of this is must run on the JVM.  So, use Ctrl-X J (with cursor at\n;;; end right paren) or Ctrl-X Ctrl-J with curso inside a form\n\n;;; Dependencies and resource requires and imports\n;;;\n(deps '[[uncomplicate/neanderthal \"0.43.1\"]\n        [criterium \"0.4.4\"]])\n\n;;; Apple is terrible.  They constantly are breaking things so we need this\n;;; now for Macs\n#_(deps '[[uncomplicate/neanderthal \"0.43.1\"\n         :exclusions \n         [[org.jcuda/jcuda-natives :classifier \"apple-x86_64\"]\n          [org.jcuda/jcublas-natives :classifier \"apple-x86_64\"]]]\n        [criterium \"0.4.4\"]])\n\n\n(require '[uncomplicate.commons.core\n           :refer [with-release let-release\n                   Releaseable release]]\n         '[uncomplicate.fluokitten.core :refer [fmap!]]\n         '[uncomplicate.neanderthal\n           [native :refer [dv dge fge dtr native-float]]\n           [core :refer [copy copy! mv! mv axpy! scal! transfer! submatrix dot\n                         transfer! transfer dim\n                         mrows ncols dot nrm2 mm mmt cols view-tr rk rk!\n                         row asum axpy rk subvector trans\n                         dia view-ge]]\n           [real :refer [entry entry!]]\n           [math :refer [signum exp sqrt cos acos]]\n           [linalg :refer [trf tri det]]\n           [vect-math :refer [fmax! tanh! mul mul! div div!\n                              linear-frac! sqrt! inv!]]])\n\n(import clojure.lang.IFn)\n\n(def product-prices [1.3 2.0 1.9 1.8])\n(def cart-vec-1 [10 0 7 3])\n(def cart-vec-2 [0 9 3 0])\n\n\"We recognize that the logic we've already developed for computing the total price matches a simple and well known mathematical operation, known as the dot product, a scalar product of two vectors.\"\n\n(defn dot-product-vec [xs ys]\n  (reduce + (map * xs ys)))\n\n\"Given two vectors, [1 2 3] and [4 5 6], the dot product computes one number, a scalar, that represent a scalar product of these two vectors. Right now, we don't even care about theoretical details of the dot product; we recognize that it technically computes the same thing that we need in our domain, and it seems useful. There are other ways to multiply vectors, which return non-scalar structures.\"\n\n(dot-product-vec [1 2 3] [4 5 6])\n\n\n\"We can see that, when applied to the vectors holding product prices and quantities, it returns the correct results that we've already seen.\"\n\n(dot-product-vec product-prices cart-vec-1)\n\n(dot-product-vec product-prices cart-vec-2)\n\n\"Getting the total price requires another map/reduce, but we will quickly see that this, too, can be generalized.\"\n\n(reduce + (map (partial dot-product-vec product-prices)\n               [cart-vec-1 cart-vec-2]))\n\n\n\"A library of linear algebra operations\nWhen we abstract away the specifics of the domain, we end up with a number of general operations that can be reused over and over, and combined into more complex, but still general, operations. Countless such operations have been studied and theoretically developed by various branches of mathematics and related applied disciplines for a long time. What's more, many have been implemented and optimized for popular hardware and software ecosystems, so our main task is to learn how to apply that vast resource to the specific domain problems.\n\nLinear algebra is particularly well supported in implementations. Whenever we need to process arrays of numbers, it is likely that at least some part of this processing, if not all of it, can be described through vector, matrix, or tensor operations.\n\nVectors\nInstead of developing our own naive implementations, we should reuse the well-defined data structures and functions provided by Neanderthal ().\n\nHere we use vectors of double precision floating point numbers to represent products' prices and carts.\"\n\n\n(def product-prices (dv [1.3 2.0 1.9 1.8]))\n(def cart-vctr-1 (dv [10 0 7 3]))\n(def cart-vctr-2 (dv [0 9 3 0]))\n\n\n\"We use the general dot function in the same way as the matching function that we had implemented before.\"\n\n(dot product-prices cart-vctr-1)\n\n(dot product-prices cart-vctr-2)\n\n\"Matrices\nOnce we start applying general operations, we can see new ways to improve our code, not so obvious at first.\n\nInstead of maintaining sequences of vectors that represent carts, and coding custom functions to process these vectors, we can put that data into the rows of a matrix. All carts are now represented by one matrix, and each row of the matrix represents one cart.\"\n\n(def carts (dge 2 4))\n\n\n\"We could have populated the matrix manually, but, since we already have the data loaded in appropriate vectors, we can copy it, showing how these structures are related.\"\n\n(copy! cart-vctr-1 (row carts 0))\n\n(copy! cart-vctr-2 (row carts 1))\n\n\"The following step is the usual opportunity for a novice to slip. Should we now iterate the rows of our newly created matrix, calling dot products on each row? No! We should recognize that the equivalent operation already exists: matrix-vector multiplication, implemented by the mv function!\n\nMost functions in this domain have short names that might sound cryptic until you get used to it. There is a method to their naming, though, and they are usually very descriptive mnemonics. For example, mv stands for Matrix-Vector multiplication. You'll guess that mm is Matrix-Matrix multiplication and so on. Like in mathematical formulas, this naming makes for code that can be viewed in a contained place that can be grasped in one view.\"\n\n(mv carts product-prices)\n\n(asum (mv carts product-prices))\n\n\"Not only that the mv operation is equivalent to multiple calls to dot, but it takes advantage of the structure of the matrix, and optimizes the computation to the available hardware. This achieves much better performance, which can compound to orders of magnitude in improvements.\n\nThese improvements materialize in more serious examples. Any implementation of a small toy problem works fast.\n\n…and more\nLet's introduce a bit more complication. Say that we want to support different discounts for each product, in the form of multipliers. That gets us the price reductions, that we should subtract from the price. An alternative way, shown in the following snippets is to subtract the discount coefficients from 1.0 to get the direct multiplier that gets us to the reduced price.\"\n\n(def discounts (dv [0.07 0 0.33 0.25]))\n(def ones (entry! (dv 4) 1))\n\n\"We can subtract two vectors by the axpy function. axpy stands for 'scalar a times x plus y'.\"\n\n(axpy -1 discounts ones)\n\n\"The mul function multiplies its vector, matrix, or tensor arguments element-wise, entry by entry.\"\n\n(mul (axpy -1 discounts ones) product-prices)\n\n\"The following code seamlessly incorporates this new part of the algorithm into the implementation that we already have.\"\n\n(asum (mv carts (mul (axpy -1 discounts ones) product-prices)))\n\n\"Why stop here? Suppose that we want to simulate the effects of multiple discount combinations on the total price. As earlier, we put all these hypothetical discount vectors into a matrix, in this case, three samples that we'd like to investigate.\"\n\n(def discount-mat (dge 4 3 [0.07 0 0.33 0.25\n                            0.05 0.30 0 0.1\n                            0 0 0.20 0.40]))\n\n\n\n\"We have to subtract these numbers from 1.0. Instead of populating the matrix with 1.0, we will demonstrate the outer product operation, implemented by the function rk. Given two vectors, it produces a matrix that holds all combinations of the product of the entries of the vectors.\"\n\n(rk ones (subvector ones 0 3))\n\n\"We can also utilize rk to \"lift\" product prices vector to a matrix whose shape matches the shape of the discount combinations matrix.\"\n\n(def ones-3 (subvector ones 0 3))\n(def discounted-prices (mul (axpy -1 discount-mat (rk ones ones-3))\n                            (rk product-prices ones-3)))\n\n\"The result is a matrix of hypothetical discount prices that we'd like to simulate.\"\n\n\"Now, the most interesting part: how do we calculate the totals from this matrix and the matrix of carts we've produced earlier. (Not) surprisingly, just a single operation, matrix multiplication, completes this task!\"\n\n(mm carts discounted-prices)\n\n\"Now we only need to sum the columns up to get the three final totals. We won't do this column-by-column. Instead, we'll use the \"mv with ones\" approach we've already encountered. Note that we need to transpose the matrix to match the desired structure.\"\n\n(trans (mm carts discounted-prices))\n\n\"And, the final answer is…\"\n\n(mv (trans (mm carts discounted-prices)) (subvector ones 0 2))\n\n\"Given three (or three million) possible discount combinations, we get a vector of the total revenue amounts. Of course, being a toy example, this code doesn't take into account that lower prices would (likely) induce more sales; let's not carry a Hello World example too far.\n\nSo, the first major benefit of using a library, such as Neanderthal (), based on many decades of numerical computing research and development is that we have access to a large treasure trove of useful, well thought, general functions for developing useful, general or customized, number processing algorithms.\n\nAnother major improvement is performance. Although toy examples may be implemented in any way you'd like, and they'd still work reasonably well, real-world data processing almost always involves either many data points, or many computation steps, or, often – both.\n\nI'm not talking about a couple dozen percentages, but improvements of many orders of magnitude. But that's a story that has to be looked at in more depth. I've written two books where I go into much, much, more depth and width on this. You can also check some earlier articles on this blog; there's many examples that demonstrate how fast this approach is!\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:end\n", :out-height "900px", :eid "ed-hello", :height "900px"}}, :specs []}} {:ed3 {:label "Euclidean and Cosine distances", :opts {:order :row, :eltsper 1, :size "auto", :wrapfn {:tid :ed3, :$split nil, :out-width "1300px", :fn [quote editor-repl-tab], :layout :left-right, :ns hwla.code, :ed-out-order :first-last, :width "730px", :md-defaults nil, :src "\n\n(defn cossim [x y]\n  (let [d (dot x y)]\n    (if (= 0.0 d)\n      0\n      (/ d (nrm2 x) (nrm2 y)))))\n\n(nrm2 (dv 1 2))\n(let [x (dv 1 2)\n      nx (nrm2 x)\n      xn (dv (repeat (dim x) nx))]\n  [nx (sqrt (dot x x))\n   (mul x x) (div x x) (div x xn) (nrm2 (div x xn))])\n\n(dim (dv 1 2))\n\n(div )(dv (repeat 2 (nrm2 (dv 1 2))))\n\n(cossim (dv 0 1 2) (dv 0 -1 -2))\n\n(dot (dv 6.6 6.2) (dv 9.7 9.9))\n\n;;; ||xi||2+||yj||2−2xi.yj\n\n(defn eucdist [x y]\n  (let [xn (nrm2 x)\n        yn (nrm2 y)\n        xy (dot x y)]\n    (sqrt (+ (* xn xn) (* yn yn) (* -2.0 xy)))))\n\n(defn eucdist-1 [x y]\n  (let [xn (nrm2 x)\n        yn (nrm2 y)\n        xn (div x (dv (repeat (dim x) (nrm2 x))))\n        yn (div y (dv (repeat (dim x) (nrm2 y))))\n        nxy (dot xn yn)]\n    (sqrt (+  1.0 1.0 (* -2.0 nxy)))))\n\n(eucdist (dv 6.6 6.2) (dv 9.7 9.9))\n\n(let [x (dv 6.6 6.2)\n      y (dv 9.7 9.9)\n      xn (div x (dv (repeat (dim x) (nrm2 x))))\n      yn (div y (dv (repeat (dim x) (nrm2 y))))\n      nxy (dot xn yn)]\n  [xn yn #_nxy (nrm2 xn) (nrm2 yn)]  )\n\n(eucdist-1 (dv 6.6 6.2) (dv 9.7 9.9))\n(eucdist-1 (dv 100.0 200.0) (dv 100.0 200.0))\n(eucdist-1 (dv 1.0 1.0) (dv 1.0 1.0))\n\n\n(defn angdist [x y]\n  (- 1 (/ (acos (cossim x y)) Math/PI)))\n\n(defn pangdist [x y]\n  (- 1 (/ (* 2.0 (acos (cossim x y))) Math/PI)))\n\n(acos (cossim (dv 0 1 4) (dv 0 1 4)))\n(angdist (dv 0 1 4) (dv 0 1 4))\n(pangdist (dv 0 1 4) (dv 0 1 4))\n(eucdist (dv 0 1 4) (dv 0 1 4))\n\n\n(def xs (dge 2 3\n             [[1 2 3]\n              [30 20 10]]\n             {:layout :row}))\n\n(mm xs (trans xs))\n\n(defn cosine-similarities [a]\n (let-release [a-dots (mmt a)]\n    (with-release [a-norms (inv! (sqrt! (copy (dia a-dots))))\n                   ab-norms (rk a-norms a-norms)]\n      (mul! (view-ge a-dots) ab-norms)\n      a-dots)))\n\n(def a (dge 4 3\n            [[1 2 3]\n             [-3 8 0]\n             [30 20 10]\n             [-7 1 -5]]\n            {:layout :row}))\n\n(cosine-similarities a)\n\n(def b (dge 4 3\n            [[1 2 3]\n             [1 2 0]\n             [0 2 3]\n             [0 1 1]]\n            {:layout :row}))\n\n(trans b)\n\n(cosine-similarities b)\n(mm b (trans b))\n\n(def c (dge 2 3 [[1 2 3] [1 2 0]] {:layout :row}))\n[c (trans c)]\n(cosine-similarities c)\n\n(mm c (trans c))\n", :out-height "900px", :eid "ed-ed3", :height "900px"}}, :specs []}}]
